{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4331068",
   "metadata": {},
   "source": [
    "## Pytorch의 두가지 주요한 특징\n",
    "\n",
    "* Numpy와 유사하지만 GPU 상에서 실행 가능한 n-차원 텐서(Tensor)\n",
    "* 신경망을 구성하고 학습하는 과정에서의 자동 미분(Automatic differentiation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24c549e",
   "metadata": {},
   "source": [
    "## Numpy를 사용한 신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c395a8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2874.528589900541\n",
      "199 1904.9289225133243\n",
      "299 1263.4433682415024\n",
      "399 839.0206003943701\n",
      "499 558.2009095060077\n",
      "599 372.3885024874886\n",
      "699 249.43486352094132\n",
      "799 168.0714930656634\n",
      "899 114.2273124130329\n",
      "999 78.59266718937205\n",
      "1099 55.00790184747686\n",
      "1199 39.39736386692083\n",
      "1299 29.06420222207438\n",
      "1399 22.22383224305969\n",
      "1499 17.695283146119806\n",
      "1599 14.696991128824669\n",
      "1699 12.711689395183734\n",
      "1799 11.397011380336828\n",
      "1899 10.52633799991306\n",
      "1999 9.9496552033279\n",
      "Result: y = 0.006266679661426255 + 0.8245379557300995x + -0.0010811065195957055x^2 + -0.08874980874116986x^3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "# 무작위로 입력과 출력 데이터를 생성\n",
    "x = np.linspace(-math.pi, math.pi, 2000)\n",
    "y = np.sin(x)\n",
    "\n",
    "\n",
    "# 무작위로 가중치 초기화\n",
    "a = np.random.randn()\n",
    "b = np.random.randn()\n",
    "c = np.random.randn()\n",
    "d = np.random.randn()\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # 순전파 단계 : 예측값 y 계산\n",
    "    # y = a + bx + cx^2 + dx^3\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "    \n",
    "    # 손실(loss)을 계산하고 출력\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "        \n",
    "    # 손실에 따른 a, b, c, d의 변화도(gradient)를 계산하고 역전파\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "    \n",
    "    # 가중치를 갱신\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "    \n",
    "print(f'Result: y = {a} + {b}x + {c}x^2 + {d}x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cfb16a",
   "metadata": {},
   "source": [
    "## Tensor를 사용한 신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25479488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 787.5328979492188\n",
      "199 537.771240234375\n",
      "299 368.5162658691406\n",
      "399 253.6938934326172\n",
      "499 175.7135467529297\n",
      "599 122.69551849365234\n",
      "699 86.60891723632812\n",
      "799 62.0188102722168\n",
      "899 45.243560791015625\n",
      "999 33.786476135253906\n",
      "1099 25.952495574951172\n",
      "1199 20.589712142944336\n",
      "1299 16.914430618286133\n",
      "1399 14.392740249633789\n",
      "1499 12.660589218139648\n",
      "1599 11.46943187713623\n",
      "1699 10.649398803710938\n",
      "1799 10.08421516418457\n",
      "1899 9.694269180297852\n",
      "1999 9.424943923950195\n",
      "Result: y = 0.021788645535707474 + 0.8435530662536621x + -0.0037589017301797867x^2 + -0.09145454317331314x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # GPU에서 실행하려면 해당 주석 제거\n",
    "\n",
    "# 무작위로 입력과 출력 데이터를 생성\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# 무작위로 가중치 초기화\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # 순전파 단계\n",
    "    y_pred = a + b*x + c*x**2 + d*x**3\n",
    "    \n",
    "    #손실(loss) 계산 및 출력\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "        \n",
    "    # 손실에 따른 a, b, c, d의 변화도(gradient) 계산하고 역전파\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred*x).sum()\n",
    "    grad_c = (grad_y_pred*x**2).sum()\n",
    "    grad_d = (grad_y_pred*x**3).sum()\n",
    "    \n",
    "    # 가중치 갱신\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "    \n",
    "    \n",
    "print(f'Result: y = {a.item()} + {b.item()}x + {c.item()}x^2 + {d.item()}x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8c6748",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "* Autograd를 사용하면, 신경망의 순전파 단계에서 연산 그래프(computational graph)를 정의한다\n",
    "* 이 그래프의 node는 tensor이고, edge는 입력 텐서로부터 출력 텐서를 만들어내는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "104087bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1592.4354248046875\n",
      "199 1107.1456298828125\n",
      "299 771.290771484375\n",
      "399 538.623046875\n",
      "499 377.28143310546875\n",
      "599 265.29412841796875\n",
      "699 187.4916534423828\n",
      "799 133.3904571533203\n",
      "899 95.73783874511719\n",
      "999 69.51084899902344\n",
      "1099 51.227691650390625\n",
      "1199 38.47236251831055\n",
      "1299 29.56684112548828\n",
      "1399 23.344715118408203\n",
      "1499 18.99447250366211\n",
      "1599 15.95096492767334\n",
      "1699 13.820333480834961\n",
      "1799 12.327840805053711\n",
      "1899 11.281784057617188\n",
      "1999 10.548212051391602\n",
      "Result: y = 0.04160797968506813 + 0.8434882164001465x + -0.007178067695349455x^2 + -0.09144531935453415\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\")\n",
    "\n",
    "# 입력값과 출력값을 갖는 텐서들 생성\n",
    "# requires_grad=False가 기본값으로 설정되어 역전파 단계 중에 이 텐서들에 대한 변화도를 계산할 필요가 없음을 나타냄\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "\n",
    "# 가중치를 갖는 임의의 텐서 생성\n",
    "# y = a + bx + cx^2 + dx^3\n",
    "# requires_grad = True로 설정하여 역전파 단계 중에 이 텐서들에 대한 변화도를 계산할 필요가 있음을 나타냄\n",
    "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # 순전파 단계\n",
    "    y_pred = a + b*x + c*x**2 + d*x**3\n",
    "    \n",
    "    \n",
    "    # 텐서들간의 연산을 사용하여 손실(loss)를 계산하고 출력\n",
    "    # 이 때 손실은 (1, ) shape를 갖는 텐서\n",
    "    # loss.item() 으로 loss가 갖고 있는 스칼라 값을 가져올 수 있음\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    # autograd를 사용하여 역전파 단계를 계산\n",
    "    # requires_grad=True를 갖는 모든 텐서들에 대한 손실의 변화도를 계산\n",
    "    loss.backward()\n",
    "    \n",
    "    # Gradient Descent를 사용하여 가중치를 직접 갱신\n",
    "    # torch.no_grad()로 감싸는 이유는, 가중치들이 requires_grad=True지만 autograd에서는 이를 추적하지 않을 것이기 때문\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "        \n",
    "        # 가중치 갱신 후에는 변화도를 직접 0으로 만듦\n",
    "        \n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "        \n",
    "print(f'Result: y = {a.item()} + {b.item()}x + {c.item()}x^2 + {d.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4311b3f",
   "metadata": {},
   "source": [
    "## Pytorch: 새 autograd Function 정의\n",
    "내부적으로, autograd의 기본(primitive) 연산자는 실제로 텐서를 조작하는 2개의 함수.\n",
    "forward 함수는 입력 텐서로부터 출력텐서를 계산\n",
    "backward 함수는 어떤 스칼라 값에 대한 출력 텐서의 변화도(gradient)를 전달\n",
    "\n",
    "이 예제에서는 y = a + bx + cx^2 + dx^3 대신 y = a + bP3(c + dx)로 모델을 정의\n",
    "여기서 P3(x)= 1/2(5x^3-3x)은 3차 르장드르 다항식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "007963a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 209.95834350585938\n",
      "199 144.66018676757812\n",
      "299 100.70249938964844\n",
      "399 71.03519439697266\n",
      "499 50.97850799560547\n",
      "599 37.403133392333984\n",
      "699 28.206867218017578\n",
      "799 21.973188400268555\n",
      "899 17.7457275390625\n",
      "999 14.877889633178711\n",
      "1099 12.931766510009766\n",
      "1199 11.610918045043945\n",
      "1299 10.714258193969727\n",
      "1399 10.10548210144043\n",
      "1499 9.692106246948242\n",
      "1599 9.411375999450684\n",
      "1699 9.220745086669922\n",
      "1799 9.091285705566406\n",
      "1899 9.003361701965332\n",
      "1999 8.943639755249023\n",
      "Result: y =-6.8844756562214116e-09 + -2.208526849746704x + 1.5037101563919464e-09x^2 | 0.2554861009120941x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class LegendrePolynomial3(torch.autograd.Function):\n",
    "    # torch.autograd.Function을 상속받아 사용자 정의 autograd Function을 구현하고, 텐서 연산을 하는 순전파 단계와 역전파 단게를 구현\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        순전파 단계에서는 입력을 갖는 텐서를 받아 출력을 갖는 텐서를 반환\n",
    "        ctx는 컨텍스트 객체(context object)로 역전파 연산을 위한 정보 저장에 사용\n",
    "        ctx.save_for_backward 메소드를 사용하여 역전파 단게에서 사용할 어떤 객체도 저장(cache) 가능\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return 0.5 * (5 * input ** 3 - 3 * input)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        역전파 단계에서는 출력에 대한 손실(loss)의 변화도(gradient)를 갖는 텐서를 받고, \n",
    "        입력에 대한 손실의 변화도를 계산해야함\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_output * 1.5 * (5 * input ** 2 - 1)\n",
    "    \n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\")\n",
    "\n",
    "# 입력값과 출력값을 갖는 텐서들을 생성\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# 가중치를 갖는 임의의 텐서를 생성\n",
    "# y = a + b*P3(c + d*x)\n",
    "# 이 가중치들이 수렴(convergence)하기 위해서는 정답으로부터 너무 멀리 떨어지지 않은 값으로 초기화가 되어야 함\n",
    "a = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 5e-6\n",
    "for t in range(2000):\n",
    "    # 사용자 정의 Function을 적용하기 위해 Function.apply 메소드 사용\n",
    "    # 여기에 'P3'라고 이름을 붙임\n",
    "    P3 = LegendrePolynomial3.apply\n",
    "    \n",
    "    # 순전파 단계\n",
    "    # 사용자 정의 autograd 연산을 사용하여 P3를 계산\n",
    "    y_pred = a + b * P3(c + d * x)\n",
    "    \n",
    "    # 손실을 계산하고 출력\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    # autograd를 사용하여 역전파 단계를 계산\n",
    "    loss.backward()\n",
    "    \n",
    "    # 경사하강법(gradient descent)을 사용하여 가중치를 갱신\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # 가중치 갱신 후에 변화도를 0으로 초기화\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "        \n",
    "print(f'Result: y ={a.item()} + {b.item()}x + {c.item()}x^2 | {d.item()}x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454802f2",
   "metadata": {},
   "source": [
    "## nn 모듈\n",
    "nn패키지는 신경망 계층(layer)와 거의 비슷한 Module의 집합을 정의\n",
    "Module은 입력 텐서를 받고 출력 텐서를 계산하는 한편, 학습 가능한 매개변수를 갖는 텐서들을 내부 상태(internal state)로 가짐\n",
    "nn 패키지는 또한 신경망을 학습시킬 때 주로 사용하는 유용한 손실 함수(loss function)들도 정의하고 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f16e0d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 323.89605712890625\n",
      "199 226.08961486816406\n",
      "299 158.8056182861328\n",
      "399 112.46723175048828\n",
      "499 80.51905822753906\n",
      "599 58.468597412109375\n",
      "699 43.233299255371094\n",
      "799 32.69586181640625\n",
      "899 25.400239944458008\n",
      "999 20.3441104888916\n",
      "1099 16.83662223815918\n",
      "1199 14.401142120361328\n",
      "1299 12.708513259887695\n",
      "1399 11.531079292297363\n",
      "1499 10.711360931396484\n",
      "1599 10.140201568603516\n",
      "1699 9.741915702819824\n",
      "1799 9.463964462280273\n",
      "1899 9.269845008850098\n",
      "1999 9.134183883666992\n",
      "Result: y = 0.01738000474870205 + 0.8500474095344543 x + -0.002998340642079711 x^2 + -0.09237831085920334 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# 이 예제에서, 출력 y는 (x, x^2, x^3)의 선형 함수이므로, 선형 계층 신경망으로 간주 할 수 있음\n",
    "# (x, x^2, x^3)를 위한 텐서를 준비\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "\n",
    "# nn 패키지를 사용하여 모델을 순차적 계층(sequence of layers)으로 정의\n",
    "# nn.Sequential은 다른 module을 포함하는 Module로, 포함되는 Module들을 순차적으로 적용하여 출력을 생성\n",
    "# 각각의 Linear Module은 선형 함수(linear function)를 사용하여 입력으로부터 출력을 계산하고, 내부 Tensor에 가중치와 편향을 저장\n",
    "# Flatten 계층은 선형 게층의 출력을 'y'의 shape과 맞도록 1D 텐서로 편다(flatten)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    y_pred = model(xx)\n",
    "    \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    model.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "            \n",
    "linear_layer = model[0]\n",
    "\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131c1cae",
   "metadata": {},
   "source": [
    "## Pytorch: optim\n",
    "Pytorch의 optim 패키지는 최적화 알고리즘에 대한 아이디어를 추상화하고 일반적으로 사용하는 최적화 알고리즘의 구현체(implementation)를 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcf0a4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 826.1966552734375\n",
      "199 306.29034423828125\n",
      "299 266.5346984863281\n",
      "399 227.42237854003906\n",
      "499 179.69998168945312\n",
      "599 129.66229248046875\n",
      "699 84.90664672851562\n",
      "799 50.38207244873047\n",
      "899 27.479511260986328\n",
      "999 14.997322082519531\n",
      "1099 10.037863731384277\n",
      "1199 8.918424606323242\n",
      "1299 9.051300048828125\n",
      "1399 8.848127365112305\n",
      "1499 8.925853729248047\n",
      "1599 9.055133819580078\n",
      "1699 8.91148567199707\n",
      "1799 8.910625457763672\n",
      "1899 8.92923641204834\n",
      "1999 8.926820755004883\n",
      "Result: y = -0.0004426115774549544 + 0.8562188744544983 x + -0.00044262089068070054 x^2 + -0.09385290741920471 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "for t in range(2000):\n",
    "    y_pred = model(xx)\n",
    "    \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "linear_layer = model[0]\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a9f90e",
   "metadata": {},
   "source": [
    "## Pytorch: 사용자 정의 nn.Module\n",
    "기존 Module의 구성(sequence)보다 더 복잡한 모델을 구성해야할 때, nn.Module의 하위 클래스로 새로운 Module을 정의하고, 입력 텐서를 받아 다른 모듈 및 autograd 연산을 사용하여 출력 텐서를 만드는 forward를 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28a87300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 349.53533935546875\n",
      "199 235.78836059570312\n",
      "299 160.0872039794922\n",
      "399 109.68301391601562\n",
      "499 76.10843658447266\n",
      "599 53.733619689941406\n",
      "699 38.815433502197266\n",
      "799 28.863845825195312\n",
      "899 22.221914291381836\n",
      "999 17.78640365600586\n",
      "1099 14.822625160217285\n",
      "1199 12.84101390838623\n",
      "1299 11.51526165008545\n",
      "1399 10.627704620361328\n",
      "1499 10.033076286315918\n",
      "1599 9.634425163269043\n",
      "1699 9.366943359375\n",
      "1799 9.187325477600098\n",
      "1899 9.066634178161621\n",
      "1999 8.985441207885742\n",
      "Result: y = -0.007527928799390793 + 0.8672866821289062 x + 0.0012986946385353804 x^2 + -0.09483043849468231 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class Polynomial3(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.randn(()))\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        텐서들 간의 임의의 연산뿐만 아니라, 생성자에서 정의한 Module을 사용할 수 있음\n",
    "        \"\"\"\n",
    "        return self.a + self.b*x + self.c * x ** 2 + self.d * x ** 3\n",
    "    \n",
    "    def string(self):\n",
    "        \"\"\"\n",
    "        Python의 다른 클래스처럼, Pytorch 모듈을 사용해서 사용자 정의 메소드를 정의할 수 있음\n",
    "        \"\"\"\n",
    "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3'\n",
    "    \n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "model = Polynomial3()\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-6)\n",
    "for t in range(2000):\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(f'Result: {model.string()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2769252c",
   "metadata": {},
   "source": [
    "## Pytorch: 제어 흐름(Control Flow) + 가중치 공유(Weight Sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5f7e0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999 3248.16796875\n",
      "3999 1496.657958984375\n",
      "5999 741.333251953125\n",
      "7999 382.6327209472656\n",
      "9999 217.3163299560547\n",
      "11999 88.50534057617188\n",
      "13999 48.05274963378906\n",
      "15999 29.503379821777344\n",
      "17999 60.692138671875\n",
      "19999 13.505494117736816\n",
      "21999 11.250572204589844\n",
      "23999 9.78499984741211\n",
      "25999 9.422563552856445\n",
      "27999 9.121001243591309\n",
      "29999 8.631287574768066\n",
      "Result: y = 0.013029329478740692 + 0.8549340963363647 x + -0.0028773683588951826 x^2 + -0.09358595311641693 x^3 + 0.00015116340364329517 x^4 ? + 0.00015116340364329517 x^5 ?\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import math\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.randn(()))\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "        self.e = torch.nn.Parameter(torch.randn(()))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.a + self.b*x + self.c * x**2 + self.d * x **3\n",
    "        for exp in range(4, random.randint(4, 6)):\n",
    "            y = y + self.e * x ** exp\n",
    "        return y\n",
    "    \n",
    "    def string(self):\n",
    "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3 + {self.e.item()} x^4 ? + {self.e.item()} x^5 ?'\n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "model = DynamicNet()\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-8, momentum=0.9)\n",
    "for t in range(30000):\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    loss = criterion(y_pred, y)\n",
    "    \n",
    "    if t % 2000 == 1999:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(f'Result: {model.string()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fccb14b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
